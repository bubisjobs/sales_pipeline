# Snowflake Data Pipeline with DBT and Airflow

This project demonstrates an end-to-end ELT (Extract, Load, Transform) data pipeline using **Snowflake**, **DBT**, and **Apache Airflow**. It processes sales order data for analytics use cases such as daily sales tracking, customer lifetime value, and employee performance.

---

## ðŸ—‚ Project Structure
```
project/
â”œâ”€â”€ airflow/                     # DAGs and orchestration
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ sales_pipeline.py    # Airflow DAG definition
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ sales_pipeline/          # DBT project folder
â”‚       â”œâ”€â”€ dbt_project.yml
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚       â”‚   â”‚   â””â”€â”€ sources.yml
â”‚       â”‚   â””â”€â”€ marts/
â”‚       â”‚       â”œâ”€â”€ mart_daily_sales.sql
â”‚       â”‚       â””â”€â”€ mart_employee_performance.sql
â”œâ”€â”€ data/                        # Raw CSV files
â”‚   â””â”€â”€ orders.csv
```

---

## ðŸ›  Tech Stack
- **Snowflake** (Cloud Data Warehouse)
- **DBT** (Transformations and Testing)
- **Apache Airflow** (Workflow orchestration)
- **Pandas + Snowflake Connector** (Python CSV loading)

---

## ðŸš€ Pipeline Overview
1. **Extract** raw CSV data (`orders.csv`)
2. **Load** it into the Snowflake `RAW` schema using a Python script
3. **Transform** with DBT into clean staging models and KPIs (marts)
4. **Schedule** and run the pipeline using Airflow

---

## ðŸ“Š Key DBT Models
### Staging (`models/staging/`):
- `stg_orders.sql`: Cleans and standardizes raw order data

### Marts (`models/marts/`):
- `mart_daily_sales.sql`: Daily revenue and order metrics
- `mart_employee_performance.sql`: Orders and sales per employee
- `mart_category_revenue.sql`: This query aggregates total revenue by product name from the staging orders table.


---

## ðŸ“„ Author
Created by Bubacarr Jobarteh


